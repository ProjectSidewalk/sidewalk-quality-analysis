{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Imports, file creation, list creations](#Imports,-file-creation-list-creations)\n",
    "* [Features for ML Model Creation](#Features-for-ML-Model-Creation)\n",
    "* [Dataframe for ML Model Creation](#Dataframe-for-ML-Model-Creation)\n",
    "* [Intersection Proximity](#Intersection-Proximity)\n",
    "* [CV Analysis](#CV-Analysis)\n",
    "* [Population Density](#Population-Density)\n",
    "* [Zone type](#Zone-type)\n",
    "* [Classification](#Classification)\n",
    "* [Analysis of min labels vs. precision](#Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Imports, file creation, list creations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import multiprocessing as mp\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import intersection_proximity\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import parse as str_parse\n",
    "import requests\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import sklearn.feature_selection\n",
    "from dateutil import parser as parser\n",
    "from imblearn.ensemble import (BalancedBaggingClassifier,\n",
    "                               BalancedRandomForestClassifier,\n",
    "                               EasyEnsembleClassifier)\n",
    "from imblearn.over_sampling import ADASYN, SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from IPython import get_ipython\n",
    "from sklearn import linear_model, svm, tree\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import (AdaBoostClassifier, BaggingClassifier,\n",
    "                              BaggingRegressor, ExtraTreesClassifier,\n",
    "                              ExtraTreesRegressor, RandomForestClassifier,\n",
    "                              RandomForestRegressor, VotingClassifier)\n",
    "from sklearn.feature_selection import RFECV, SelectFromModel, VarianceThreshold\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix, precision_score,\n",
    "                             r2_score, recall_score)\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import (KFold, StratifiedKFold, cross_val_score,\n",
    "                                     train_test_split)\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "from region_stats import RegionStats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads all of the data that we need\n",
    "users_old = pd.read_csv('ml-users.csv')\n",
    "users = pd.read_csv('users_one_mission.csv')\n",
    "# users = pd.read_csv('cv-accuracy.csv')\n",
    "labels = pd.read_csv('ml-label-correctness-one-mission.csv')\n",
    "grouped_labels = labels.groupby('user_id')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the user that meet a certain criteria to analyze\n",
    "to_remove = []\n",
    "for index, current in enumerate(users['labels_validated']):\n",
    "    if current < 16:\n",
    "        to_remove.append(index)\n",
    "users.drop(users.index[to_remove], inplace=True)\n",
    "users = users.reset_index(drop = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists of all the tags belonging to each label type\n",
    "curb_ramp_tags = ['narrow', 'points into traffic', 'missing friction strip', 'steep', 'not enough landing space']\n",
    "obstacle_tags = ['fire hydrant', 'pole', 'tree', 'vegetation', 'trash/recycling can', 'parked car', 'parked bike']\n",
    "missing_curb_ramp_tags = ['alternate route present', 'no alternate route', 'unclear if needed']\n",
    "surface_problem_tags = ['bumpy', 'uneven', 'cracks', 'grass', 'narrow sidewalk']\n",
    "no_sidewalk_tags = ['ends abruptly', 'street has a sidewalk', 'street has no sidewalks']\n",
    "other_tags = ['missing crosswalk', 'no bus stop access']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Run the next two cells just once (warning: will take a super long time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes in the row and appends the row into the given file\n",
    "def file_appender(row, created, user_info):\n",
    "    if user_info not in created:\n",
    "        with open('{0}_new.csv'.format(user_info), 'w', newline = '') as new_user:\n",
    "            writer = csv.writer(new_user)\n",
    "            created.append(user_info)\n",
    "            writer.writerow(header)\n",
    "    with open('{0}_new.csv'.format(user_info), 'a', newline = '') as edit_user:\n",
    "        editor = csv.writer(edit_user)\n",
    "        editor.writerow(row)\n",
    "    return created\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parses through the csv and creates new csv for every user & action\n",
    "# This is needed to be able to avoid storing all of the interaction information as a variable\n",
    "# These csv files will be referenced in future analysis\n",
    "# Please unzip ml-interactions.tar.gz\n",
    "with open('ml-interactions.csv', newline = '') as data:\n",
    "    reader = csv.reader(data)\n",
    "    header = []\n",
    "    users_created = []\n",
    "    events_created = []\n",
    "    for row in reader:\n",
    "        if row[1] == 'user_id':\n",
    "            header = row\n",
    "        else:\n",
    "            user_id = row[1]\n",
    "            current_event = row[4]\n",
    "            users_created = file_appender(row, users_created, user_id)\n",
    "            events_created = file_appender(row, events_created, current_event)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parses through each user's csv and figures out how many unique panos and missions seen\n",
    "# This is used to normalize the data to be by per missions or per pano\n",
    "user_panos = {}\n",
    "user_missions = {}\n",
    "for current_user in users['user_id']:\n",
    "    df_current = pd.read_csv('{0}_new.csv'.format(current_user))\n",
    "    user_panos[current_user] = df_current['gsv_panorama_id'].nunique()\n",
    "    user_missions[current_user] = df_current['mission_id'].nunique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "panos_seen = ['panos seen']\n",
    "df_user_panos = pd.DataFrame(list(user_panos.values()), columns = panos_seen, index = user_panos.keys())\n",
    "df_user_panos.to_csv('panos_seen.csv', encoding='utf-8', index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes all label data from a csv and creates csv of each user's calculated label data\n",
    "#  Including number of correct and false labels, total labels and number of validated labels\n",
    "user_labels = {}\n",
    "labels_grouped = labels.groupby('user_id')\n",
    "for current_id, current_group in labels_grouped:\n",
    "    total = len(current_group)\n",
    "    accuracy = 0\n",
    "    correct = sum(current_group['correct'] == 't')\n",
    "    false = sum(current_group['correct'] == 'f')\n",
    "    validated = correct + false\n",
    "    if validated != 0:\n",
    "        accuracy = float(correct) / float(validated) * 100\n",
    "    missions_completed = user_missions[current_id]\n",
    "    user_labels[current_id] = current_id, total, validated, correct, false,  accuracy, missions_completed\n",
    "header = ['user_id', 'total_labels', 'labels_validated', 'correct_labels', 'false_labels', 'accuracy', 'missions_completed']\n",
    "with open('users_one_mission.csv', 'w', newline = '') as new_user:\n",
    "    writer = csv.writer(new_user)\n",
    "    writer.writerow(header)\n",
    "for user in user_labels.values():\n",
    "    with open('users_one_mission.csv', 'a', newline = '') as edit_user:\n",
    "        editor = csv.writer(edit_user)\n",
    "        editor.writerow(user)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Features for ML Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a lists of each user's outputs (Quality 0 = Bad, 1 = Good, and accuracy)\n",
    "# Also creates an list for features\n",
    "# for the DC data, I don't have an 'accuracy' because that is based off of Seattel validations\n",
    "# so their precision, recall, and worker type is recorded\n",
    "def create_groups():\n",
    "    for index, entry in enumerate(users['user_id']):\n",
    "        current_accuracy = users['accuracy'][index]\n",
    "#         current_recall = users['recall'][index]\n",
    "#         current_precision = users['precision'][index]\n",
    "#         current_f.measure = users['f.measure'][index]\n",
    "#         worker_type = users['worker.type'][index]\n",
    "        if current_accuracy <= 65:\n",
    "            quality = '0'\n",
    "        else:\n",
    "            quality = '1'\n",
    "        all_output.append([quality, current_accuracy])\n",
    "        all_data[entry] = [0]\n",
    "        all_id.append(entry)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds a feature into the features list of all of the data points\n",
    "# Feature is the name of the user action that is recorded\n",
    "# info_type is the metric of how the feature is calculated\n",
    "# feature_title is the name of the feature\n",
    "# multi is if the feature should be normalized to be measured by missions & panos\n",
    "def add_feature(feature, info_type, feature_title, multi):\n",
    "    if feature_title not in all_feature_header:\n",
    "        for index, entry in enumerate(users['user_id']):\n",
    "            df_user = pd.read_csv('{0}_new.csv'.format(entry))\n",
    "            if info_type == 'count':\n",
    "                if multi == True:\n",
    "                    by_pano = df_user.groupby(('mission_id', 'gsv_panorama_id')).apply(lambda x: sum(x['action'] == feature))\n",
    "                    pano_mean = by_pano.mean()\n",
    "                    pano_std = by_pano.std()\n",
    "                    by_mission = df_user.groupby('mission_id').apply(lambda x: sum(x['action'] == feature))\n",
    "                    mission_mean = by_mission.mean()\n",
    "                    mission_std = by_mission.std()\n",
    "                else:\n",
    "                    info = action_count[feature] \n",
    "            elif info_type == 'pitch':\n",
    "                info = df_user['pitch'].mean()     \n",
    "            elif info_type == 'heading':\n",
    "                df_grouped = df_user.groupby(['gsv_panorama_id'])\n",
    "                current_heading = []\n",
    "                for current, group in df_grouped:\n",
    "                    current_heading.append(group['heading'].max() - group['heading'].min())\n",
    "                info = (sum(current_heading) / float(len(current_heading))) \n",
    "            elif info_type == 'heading 350':\n",
    "                df_grouped = df_user.groupby(['gsv_panorama_id'])\n",
    "                full_heading_count = 0\n",
    "                for current, group in df_grouped:\n",
    "                    range = group['heading'].max() - group['heading'].min()\n",
    "                    if range >= 350:\n",
    "                        full_heading_count += 1\n",
    "                info = full_heading_count / float(user_panos[entry])\n",
    "            else:\n",
    "                pano_mean = 0\n",
    "                pano_std = 0\n",
    "                mission_mean = 0\n",
    "                mission_std = 0\n",
    "                info = 0\n",
    "            if multi:\n",
    "                all_data[entry].append(mission_mean)\n",
    "                all_data[entry].append(pano_mean)\n",
    "                all_data[entry].append(mission_std)\n",
    "                all_data[entry].append(pano_std)\n",
    "            else:\n",
    "                all_data[entry].append(info)\n",
    "        if multi:\n",
    "            all_feature_header.append(feature_title + ' per Mission')\n",
    "            all_feature_header.append(feature_title + ' per Pano')\n",
    "            all_feature_header.append(feature_title + ' Standard Deviation per Mission')\n",
    "            all_feature_header.append(feature_title + ' Standard Deviation per Pano')\n",
    "        else:\n",
    "            all_feature_header.append(feature_title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds all of the users interaction data into a list of lists\n",
    "# only needed for analysis of min panos vs. precision\n",
    "def feature_data(feature, info_type, feature_title, multi):\n",
    "    if feature_title not in all_feature_header:\n",
    "        for index, entry in enumerate(users['user_id']):\n",
    "            df_user = pd.read_csv('{0}_new.csv'.format(entry))\n",
    "            if info_type == 'count':\n",
    "                info = df_user.groupby(('mission_id', 'gsv_panorama_id')).apply(lambda x: sum(x['action'] == feature))\n",
    "                info = info.tolist()\n",
    "            elif info_type == 'pitch':\n",
    "                info = df_user['pitch']     \n",
    "            elif info_type == 'heading':\n",
    "                df_grouped = df_user.groupby(['gsv_panorama_id'])\n",
    "                current_heading = []\n",
    "                for current, group in df_grouped:\n",
    "                    current_heading.append(group['heading'].max() - group['heading'].min())\n",
    "                info = current_heading\n",
    "            elif info_type == 'heading 350':\n",
    "                df_grouped = df_user.groupby(['gsv_panorama_id'])\n",
    "                info = []\n",
    "                for current, group in df_grouped:\n",
    "                    range = group['heading'].max() - group['heading'].min()\n",
    "                    if range >= 350:\n",
    "                        info.append(1)\n",
    "                    else :\n",
    "                        info.append(0)\n",
    "            else:\n",
    "                info = 0\n",
    "            all_data[entry].append(info)\n",
    "        all_feature_header.append(feature_title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates empty lists to hold all of the user data\n",
    "all_data = {}\n",
    "all_id = []\n",
    "all_output = []\n",
    "all_feature_header = ['test']\n",
    "output_header = ['Quality', 'accuracy']\n",
    "create_groups()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds these features into the lists (mean & standard deviation,for per pano and per mission)\n",
    "add_feature( None, 'heading', 'Average Heading Range', False)\n",
    "add_feature( None, 'heading 350', 'Panos w/ over 350 Degrees seen', False)\n",
    "add_feature( None, 'pitch', 'Average Pitch', False)\n",
    "add_feature( 'ContextMenu_TagAdded', 'count', 'Tags Added', True)\n",
    "add_feature( 'LabelingCanvas_FinishLabeling', 'count', 'Labels Confirmed', True)\n",
    "add_feature( 'RemoveLabel', 'count', 'Labels Removed', True)\n",
    "add_feature( 'LowLevelEvent_keydown', 'count', 'Key Presses', True)\n",
    "add_feature( 'LowLevelEvent_mousedown', 'count', 'Mouse Clicks', True)\n",
    "add_feature( 'Click_ZoomIn', 'count', 'Zoom', True)\n",
    "add_feature( 'ContextMenu_TextBoxChange', 'count', 'Comments Written', True)\n",
    "add_feature( 'LowLevelEvent_mousemove', 'count', 'Mouse Movements', True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds these interaction data in to a lists (Creates a lists of lists)\n",
    "# only needed for analysis of min panos vs. precision\n",
    "feature_data( None, 'heading', 'Average Heading Range', False)\n",
    "feature_data( None, 'heading 350', 'Panos w/ over 350 Degrees seen', False)\n",
    "feature_data( None, 'pitch', 'Average Pitch', False)\n",
    "feature_data( 'ContextMenu_TagAdded', 'count', 'Tags Added', True)\n",
    "feature_data( 'LabelingCanvas_FinishLabeling', 'count', 'Labels Confirmed', True)\n",
    "feature_data( 'RemoveLabel', 'count', 'Labels Removed', True)\n",
    "feature_data( 'LowLevelEvent_keydown', 'count', 'Key Presses', True)\n",
    "feature_data( 'LowLevelEvent_mousedown', 'count', 'Mouse Clicks', True)\n",
    "feature_data( 'Click_ZoomIn', 'count', 'Zoom', True)\n",
    "feature_data( 'ContextMenu_TextBoxChange', 'count', 'Comments Written', True)\n",
    "feature_data( 'LowLevelEvent_mousemove', 'count', 'Tags Added', True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Dataframe for ML Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates dataframes of the list of users features and outputs as the data with the \n",
    "# Column headers being the list of feature names / output names\n",
    "df_all = pd.DataFrame(list(all_data.values()), columns = all_feature_header, index = all_id)\n",
    "df_all_output = pd.DataFrame(all_output, columns = output_header, index = all_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drops any unwanted column\n",
    "df_all = df_all.drop(columns=['test'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates csvs out of the dataframes\n",
    "df_all.to_csv('all_users.csv', encoding='utf-8', index=True)\n",
    "df_all_output.to_csv('all_users_new.csv', encoding='utf-8', index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Load all the user information and label correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# users = pd.read_csv('ml-users.csv')\n",
    "users = pd.read_csv('users_one_mission.csv')\n",
    "users = users.set_index('user_id')\n",
    " \n",
    "point_labels = pd.read_csv('sidewalk-seattle-label_point.csv')\n",
    "point_labels.set_index('label_id', inplace=True)\n",
    " \n",
    "# label_correctness = pd.read_csv('ml-label-correctness.csv')\n",
    "label_correctness = pd.read_csv('ml-label-correctness-one-mission.csv')\n",
    " \n",
    "label_correctness.set_index('label_id', inplace=True)\n",
    " \n",
    "label_correctness = label_correctness.join(point_labels)\n",
    " \n",
    "label_correctness = label_correctness[['user_id', 'label_type', \n",
    "    'correct', 'sv_image_x', 'sv_image_y', 'canvas_x', 'canvas_y', \n",
    "    'heading', 'pitch', 'zoom', 'lat', 'lng']]\n",
    "\n",
    " \n",
    "users_for_analysis = users.index[users['labels_validated'] > 0]\n",
    "label_correctness = label_correctness[label_correctness['user_id'].isin(users_for_analysis)]\n",
    "users = users.loc[users_for_analysis]\n",
    " \n",
    "label_correctness.update(label_correctness['correct'][~pd.isna(label_correctness['correct'])] == 't')\n",
    " \n",
    "label_type_encoder = OrdinalEncoder()\n",
    " \n",
    "label_correctness['label_type'] = label_type_encoder.fit_transform(label_correctness[['label_type']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Intersection Proximity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Load intersection proximity for each label\n",
    "\n",
    " proximity_distance is the absolute distance to the nearest intersection\n",
    "\n",
    " proximity_middleness is the \"middleness\" as measured in the intersection proximity library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = intersection_proximity.IntersectionProximity(intersection_proximity.default_settings['seattle'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proximity_info(label):\n",
    "    try:\n",
    "        distance, middleness = ip.compute_proximity(label.lat, label.lng, cache=True)\n",
    "    except Exception:\n",
    "        distance = -1\n",
    "        middleness = -1\n",
    "    \n",
    "    return pd.Series({\n",
    "        'proximity_distance': distance,\n",
    "        'proximity_middleness': middleness\n",
    "    })\n",
    "\n",
    "label_correctness = label_correctness.join(label_correctness.apply(get_proximity_info, axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # CV Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This is the full analysis of using CV predictions for classifying labels.\n",
    "\n",
    " We found that it's not very useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_predictions = pd.read_csv('summary_user.csv').rename(columns={\n",
    "    'CVLabel': 'cv_label_type',\n",
    "    'Confidence': 'cv_confidence'\n",
    "},)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_types = ['CurbRamp', 'NoCurbRamp', 'Obstacle', 'SurfaceProblem']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_correctness = label_correctness.join(cv_predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(8, 8))\n",
    "for i in range(len(label_types)):\n",
    "    ax = axes[i//2][i%2]\n",
    "    label_encoded = label_type_encoder.transform([[label_types[i]]])[0][0]\n",
    "    selection = label_correctness[~pd.isna(label_correctness['correct']) & ~pd.isna(label_correctness['cv_confidence']) & (label_correctness['label_type'] == label_encoded)]\n",
    "    ax.set_xlabel('CV Confidence')\n",
    "    ax.set_ylabel('relative count')\n",
    "    ax.set_title(label_types[i])\n",
    "    ax.hist(selection[selection['correct'].astype(bool)]['cv_confidence'], alpha=0.5, label='correct', density=True)\n",
    "    ax.hist(selection[~selection['correct'].astype(bool)]['cv_confidence'], alpha=0.5, label='incorrect', density=True)\n",
    "    ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_correctness.loc[:, 'cv_label_type'][~pd.isna(label_correctness.loc[:, 'cv_label_type'])] =     label_type_encoder.transform(pd.DataFrame(label_correctness.loc[:, 'cv_label_type'][~pd.isna(label_correctness.loc[:, 'cv_label_type'])]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = np.zeros((4, 4))\n",
    "for i in range(len(label_types)):\n",
    "    for j in range(len(label_types)):\n",
    "        i_encoded = label_type_encoder.transform([[label_types[i]]])[0][0]\n",
    "        j_encoded = label_type_encoder.transform([[label_types[j]]])[0][0]\n",
    "\n",
    "        selection = label_correctness[~pd.isna(label_correctness['correct']) \n",
    "            & (label_correctness['label_type'] == i_encoded)\n",
    "            & (label_correctness['cv_label_type'] == j_encoded)]\n",
    "        \n",
    "        try:\n",
    "            prob[i][j] = np.sum(selection['correct']) / len(selection)\n",
    "        except ZeroDivisionError:\n",
    "            prob[i][j] = np.nan\n",
    "\n",
    "prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Population Density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Analysis of population density as a feature. It has some correlation with accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = RegionStats('data_seattle.geojson')\n",
    "label_correctness = label_correctness.join(\n",
    "    label_correctness.apply(lambda x: pd.Series(rs.get_properties(x.lng, x.lat)), axis=1)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_all = label_correctness[['density', 'correct', 'label_type']]\n",
    "selection_all = selection_all[~pd.isna(selection_all).any(axis=1)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(8, 8))\n",
    "fig2, axes2 = plt.subplots(nrows=2, ncols=2, figsize=(8, 8))\n",
    "\n",
    "for i in range(len(label_types)):\n",
    "    ax = axes[i//2][i%2]\n",
    "    label_encoded = label_type_encoder.transform([[label_types[i]]])[0][0]\n",
    "    selection = selection_all[selection_all['label_type'] == label_encoded]\n",
    "    ax.set_xlabel('Population density (people/sq. mile)')\n",
    "    ax.set_ylabel('count')\n",
    "    ax.set_title(label_types[i])\n",
    "    nc, bins, _ = ax.hist(selection[selection['correct'] == True]['density'], density=False, bins=20, alpha=0.5, label='correct')\n",
    "    ni, _, _ = ax.hist(selection[selection['correct'] == False]['density'], density=False, bins=bins, alpha=0.5, label='incorrect')\n",
    "    ax.legend()\n",
    "\n",
    "    ax2 = axes2[i//2][i%2]\n",
    "    ax2.set_xlabel('Population density (people/sq. mile)')\n",
    "    ax2.set_ylabel('probability correct')\n",
    "    ax2.set_title(label_types[i])\n",
    "    density_vals = (bins[:-1] + bins[1:])/2\n",
    "    correct_prob = nc / (ni + nc)\n",
    "    mask = ~np.isnan(correct_prob)\n",
    "    density_vals = density_vals[mask]\n",
    "    correct_prob = correct_prob[mask]\n",
    "    ax2.scatter(density_vals, correct_prob)\n",
    "\n",
    "    \n",
    "    z = np.polyfit(density_vals, correct_prob, 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax2.plot(density_vals, p(density_vals), \n",
    "    label=f\"R={r2_score(correct_prob, p(density_vals)):.3f}\")\n",
    "\n",
    "    ax2.legend()\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "fig2.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Zone type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Zone type is even better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = RegionStats('Zoning_Detailed.geojson')\n",
    "label_correctness = label_correctness.join(\n",
    "    label_correctness.apply(lambda x: pd.Series(rs.get_properties(x.lng, x.lat)), axis=1)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection_all = label_correctness[['CATEGORY_DESC', 'correct', 'label_type']]\n",
    "selection_all = selection_all[~pd.isna(selection_all).any(axis=1)]\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(8, 10))\n",
    "categories = selection_all['CATEGORY_DESC'].unique()\n",
    "\n",
    "for i in range(len(label_types)):\n",
    "    ax = axes[i//2][i%2]\n",
    "    label_encoded = label_type_encoder.transform([[label_types[i]]])[0][0]\n",
    "    selection = selection_all[selection_all['label_type'] == label_encoded]\n",
    "    # ax.set_xlabel('Population density (people/sq. mile)')\n",
    "    # ax.set_ylabel('count')\n",
    "    ax.set_title(label_types[i])\n",
    "    prob_correct = dict()\n",
    "    for category in categories:\n",
    "        if np.sum(selection['CATEGORY_DESC'] == category) > 100:\n",
    "            prob_correct[category] = np.mean(selection['correct'][selection['CATEGORY_DESC'] == category])\n",
    "        else:\n",
    "            prob_correct[category] = 0\n",
    "        # num_in[category] = np.sum(selection['CATEGORY_DESC'] == category)\n",
    "\n",
    "    ax.bar(prob_correct.keys(), prob_correct.values())\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.tick_params(axis='x', labelrotation=90)\n",
    "\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This is the basic pipeline for classification:\n",
    "\n",
    " The set of training users is split, 35% into a training set for the label classifier and 65% into a testing set for the accuracy classifier.\n",
    "\n",
    " All the label validations from the 35% are used to train the classifier, using the features in `features`.\n",
    "\n",
    " Then, the label classifier predicts p(correct) for all the labels in the 65% set. `prob_hist` is used to extract a small number of features from the list of probabilities.\n",
    "\n",
    " The accuracy classifier is trained on the features from `prob_hist` + the user features generated in Tyler's notebook for the 65% set.\n",
    "\n",
    " To test the final classifier, we do the same as the 65% test, but with just the testing users: we pass the results of the label classifier into `prob_hist`, add the features from Tyler's notebook, and then pass that in to the accuracy classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_hist(probabilities, n_bins=5):\n",
    "    return [np.mean(probabilities), np.std(probabilities),\n",
    "        np.percentile(probabilities, 25), np.percentile(probabilities, 50),\n",
    "        np.percentile(probabilities, 75), np.mean(probabilities[(probabilities > 0.25) | (probabilities < 0.75)])]\n",
    "def dearray(array):\n",
    "    return np.array([list(l) for l in array])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Load Tyler's features (see his notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_quality_features = pd.read_csv('all_users.csv')\n",
    "user_quality_features.set_index('user_id', inplace=True)\n",
    "# user_quality_features.set_index('Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_filtered = users[users['labels_validated'] > 25]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'classification'\n",
    "# mode = 'regression'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['label_type', 'sv_image_y', 'canvas_x', 'canvas_y', 'heading', 'pitch', 'zoom', 'lat', 'lng', 'proximity_distance', 'proximity_middleness', 'CLASS_DESC', 'ZONEID']\n",
    "\n",
    "proportion_labels = 0.35\n",
    "comparisons = pd.DataFrame()\n",
    "split_num = 0\n",
    "np.random.seed(0)\n",
    "\n",
    "for train_index, test_index in KFold(n_splits=5, shuffle=True, random_state=0).split(users_filtered.index):\n",
    "    X_train, X_test = users_filtered.index[train_index], users_filtered.index[test_index]\n",
    "    \n",
    "    if mode == 'classification':\n",
    "        y_train, y_test = users_filtered['accuracy'][train_index] > 65, users_filtered['accuracy'][test_index] > 65\n",
    "    else:\n",
    "        y_train, y_test = users_filtered['accuracy'][train_index], users_filtered['accuracy'][test_index]\n",
    "\n",
    "#     y_train, y_test = users_filtered['missions_completed'][train_index] > 6, users_filtered['missions_completed'][test_index] > 6\n",
    "\n",
    "    mask = np.random.permutation(np.arange(len(X_train)))\n",
    "    users_labels_train = X_train[mask[:int(proportion_labels * len(mask))]]\n",
    "    users_labels_test = X_train[mask[int(proportion_labels * len(mask)):]]\n",
    "\n",
    "    train_labels = label_correctness[label_correctness['user_id'].isin(X_train)].copy()\n",
    "    test_labels = label_correctness[label_correctness['user_id'].isin(X_test)].copy()\n",
    "    #%%\n",
    "    test_labels = test_labels.drop(columns='correct')\n",
    "    #%%\n",
    "    train_labels = train_labels[~pd.isna(train_labels['correct'])]\n",
    "    train_labels = train_labels[~(pd.isna(train_labels[features]).any(axis=1))]\n",
    "    test_labels = test_labels[~(pd.isna(test_labels[features]).any(axis=1))]\n",
    "    # scaler = StandardScaler()\n",
    "    # train_labels[features] = scaler.fit_transform(train_labels[features])\n",
    "    \n",
    "    en = OrdinalEncoder()\n",
    "    en.fit(pd.concat((train_labels[['CLASS_DESC']], test_labels[['CLASS_DESC']])))\n",
    "    train_labels[['CLASS_DESC']] = en.transform(train_labels[['CLASS_DESC']])\n",
    "    test_labels[['CLASS_DESC']] = en.transform(test_labels[['CLASS_DESC']])\n",
    "\n",
    "    #%%\n",
    "    rfe_labels = RFECV(estimator=RandomForestClassifier(n_estimators=10), step=1, cv=StratifiedKFold(5),\n",
    "           scoring='precision')\n",
    "    clf_labels = RandomForestClassifier(random_state=0, n_jobs=-1, n_estimators=30)\n",
    "    # clf_accuracy = BalancedBaggingClassifier(n_jobs=-1, random_state=0, n_estimators=100)\n",
    "    rfe_accuracy = RFECV(estimator=RandomForestClassifier(n_estimators=10), step=1, cv=StratifiedKFold(5), scoring='f1')\n",
    "    \n",
    "    if mode == 'classification':\n",
    "        clf_accuracy = BalancedBaggingClassifier(random_state=0, n_jobs=-1, n_estimators=30)\n",
    "        rfe_accuracy = RFECV(estimator=RandomForestClassifier(n_estimators=10), step=1, cv=StratifiedKFold(5), scoring='f1')\n",
    "        \n",
    "    else:\n",
    "        clf_accuracy = BaggingRegressor(random_state=0, n_jobs=-1, n_estimators=30)\n",
    "        rfe_accuracy = RFECV(estimator=RandomForestClassifier(n_estimators=10), step=1, cv=StratifiedKFold(5), scoring='f1')\n",
    "\n",
    "    # clf = BalancedRandomForestClassifier(random_state=0)  \n",
    "    \n",
    "    # TODO don't eliminate all nans\n",
    "\n",
    "    #%%\n",
    "    # clf_labels.fit(train_labels[features], train_labels['correct'].astype(int))\n",
    "    print('Training label classifier...')\n",
    "    rfe_labels.fit(train_labels[train_labels['user_id'].isin(users_labels_train)][features].values, \n",
    "        train_labels[train_labels['user_id'].isin(users_labels_train)]['correct'].astype(int))\n",
    "    \n",
    "    clf_labels.fit(train_labels[train_labels['user_id'].isin(users_labels_train)][features].values[:, rfe_labels.support_], \n",
    "        train_labels[train_labels['user_id'].isin(users_labels_train)]['correct'].astype(int))\n",
    "    \n",
    "    train_labels = train_labels.join(pd.Series(\n",
    "        data=clf_labels.predict_proba(train_labels[train_labels['user_id'].isin(users_labels_test)][features].values[:, rfe_labels.support_])[:, 1], \n",
    "        index=train_labels[train_labels['user_id'].isin(users_labels_test)].index).rename('prob'), how='outer')\n",
    "    \n",
    "    prob_hist_predictions = pd.DataFrame(train_labels[train_labels['user_id'].isin(users_labels_test)]\n",
    "        .groupby('user_id').apply(lambda x:\\\n",
    "        prob_hist(x['prob'].values)).rename('prob'))\n",
    "    \n",
    "    prob_hist_predictions = prob_hist_predictions.join(user_quality_features)\n",
    "    \n",
    "    print('Training accuracy classifier...')\n",
    "    if mode == 'classification':\n",
    "        rfe_accuracy.fit(np.concatenate((dearray(prob_hist_predictions['prob']), \n",
    "            prob_hist_predictions.drop(columns='prob').values), axis=1), \n",
    "            y_train.loc[prob_hist_predictions.index])\n",
    "    else:\n",
    "        rfe_accuracy.fit(np.concatenate((dearray(prob_hist_predictions['prob']), \n",
    "            prob_hist_predictions.drop(columns='prob').values), axis=1), \n",
    "            y_train.loc[prob_hist_predictions.index] > 65)\n",
    "    \n",
    "    clf_accuracy.fit(np.concatenate((dearray(prob_hist_predictions['prob']), \n",
    "        prob_hist_predictions.drop(columns='prob').values), axis=1)[:, rfe_accuracy.support_], \n",
    "        y_train.loc[prob_hist_predictions.index])\n",
    "    #%%\n",
    "    # Probabililty correct\n",
    "    useful_test = test_labels[~pd.isna(test_labels[features]).any(axis=1)].copy()  # TODO don't eliminate all nans\n",
    "    # useful_test[features] = scaler.transform(useful_test[features])\n",
    "    # useful_test = useful_test.join(useful_test.apply(get_proximity_info, axis=1))\n",
    "    useful_test.loc[:, 'prob'] = clf_labels.predict_proba(useful_test[features].values[:, rfe_labels.support_])[:, 1]\n",
    "\n",
    "    # a = useful_test.groupby('user_id').apply(lambda x: prob_hist(x['prob']))\n",
    "    # break\n",
    "    #%%\n",
    "\n",
    "    # Now predict accuracy\n",
    "\n",
    "    def predict_accuracy(probs, features):\n",
    "        # fig = plt.figure()\n",
    "        # plt.xlim(0, 1)\n",
    "        # plt.hist(probs)\n",
    "        \n",
    "        # selected_probs = probs[~np.isnan(probs)]\n",
    "        # return np.mean(selected_probs)\n",
    "        # return clf_accuracy.predict_proba([np.concatenate((prob_hist(probs), features))])[:, 1][0]\n",
    "        return clf_accuracy.predict([np.concatenate((prob_hist(probs), features))[rfe_accuracy.support_]])[0]\n",
    "    \n",
    "    print('Making predictions...')\n",
    "    mean_probs = useful_test.groupby('user_id').apply(lambda x: predict_accuracy(x['prob'].values, user_quality_features.loc[x.name])).rename('predicted')\n",
    "\n",
    "    #%%\n",
    "    comparison = pd.DataFrame((mean_probs, y_test, pd.Series(np.full((len(y_test)), split_num), name='split_num', index=y_test.index))).T\n",
    "    comparison['prob_hist'] = useful_test.groupby('user_id').apply(lambda x: prob_hist(x['prob'].values))\n",
    "    comparison['probs'] = useful_test.groupby('user_id').apply(lambda x: x['prob'].values)\n",
    "    comparisons = comparisons.append(comparison)\n",
    "\n",
    "    #%%\n",
    "\n",
    "    split_num += 1\n",
    "\n",
    "    # sys.stderr.write(f'{split_num} / 5\\n')\n",
    "\n",
    "# comparisons['accuracy'] = (comparisons['accuracy']).astype(int)\n",
    "\n",
    "if mode == 'classification':\n",
    "    mask = ~pd.isna(comparisons[['accuracy', 'predicted']]).any(axis=1)\n",
    "    print(precision_score(comparisons['accuracy'][mask], comparisons['predicted'][mask]))\n",
    "    print(recall_score(comparisons['accuracy'][mask], comparisons['predicted'][mask]))\n",
    "    print(accuracy_score(comparisons['accuracy'][mask], comparisons['predicted'][mask]))\n",
    "    print(confusion_matrix(comparisons['accuracy'][mask], comparisons['predicted'][mask]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If running a regression, use this to view the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons = comparisons[~pd.isna(comparisons[['predicted', 'accuracy']]).any(axis=1)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons['predicted'].max()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(users['labels_validated'] > 25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "r2 = scipy.stats.pearsonr(comparisons['accuracy'], comparisons['predicted'])\n",
    "ax.scatter(comparisons['predicted'], comparisons['accuracy'], )\n",
    "z = np.polyfit(comparisons['predicted'],comparisons['accuracy'], 1)\n",
    "w = np.poly1d(z)\n",
    "ax.plot(comparisons['predicted'], w(comparisons['predicted']), color='red', label=f'R={r2[0]:.3f}\\n')\n",
    "ax.set_xlim((0, 100))\n",
    "ax.set_ylim((0, 100))\n",
    "ax.set_xlabel('predicted accuracy')\n",
    "ax.set_ylabel('actual accuracy')\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " View the results of RFE using this. You can change `rfe` to either `rfe_labels` or `rfe_accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = rfe_accuracy\n",
    "print(\"Optimal number of features : %d\" % rfe.n_features_)\n",
    "print(\"Optimal number of features : %s\" % str(rfe.support_))\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfe.grid_scores_) + 1), rfe.grid_scores_)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Analysis of min labels vs. precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This takes about 30 mins to run. but there are some generated plots in GitHub. We found that >45 validated labels is optimal for the performance of the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_r(users, label_correctness):\n",
    "    features = ['label_type', 'sv_image_y', 'canvas_x', 'canvas_y', 'heading', 'pitch', 'zoom', 'lat', 'lng',]\n",
    "    proportion_labels = 0.1\n",
    "    comparisons = pd.DataFrame()\n",
    "    split_num = 0\n",
    "    np.random.seed(0)\n",
    "\n",
    "    for train_index, test_index in KFold(n_splits=5, shuffle=True, random_state=0).split(users.index):\n",
    "        X_train, X_test = users.index[train_index], users.index[test_index]\n",
    "        y_train, y_test = users['accuracy'][train_index], users['accuracy'][test_index]\n",
    "\n",
    "        mask = np.random.permutation(np.arange(len(X_train)))\n",
    "        users_labels_train = X_train[mask[:int(proportion_labels * len(mask))]]\n",
    "        users_labels_test = X_train[mask[int(proportion_labels * len(mask)):]]\n",
    "\n",
    "        train_labels = label_correctness[label_correctness['user_id'].isin(X_train)]\n",
    "        test_labels = label_correctness[label_correctness['user_id'].isin(X_test)]\n",
    "        #%%\n",
    "        test_labels = test_labels.drop(columns='correct')\n",
    "        #%%\n",
    "        train_labels = train_labels[~pd.isna(train_labels['correct'])]\n",
    "        train_labels = train_labels[~(pd.isna(train_labels[features]).any(axis=1))]\n",
    "        # scaler = StandardScaler()\n",
    "        # train_labels[features] = scaler.fit_transform(train_labels[features])\n",
    "\n",
    "        #%%\n",
    "        rfe_labels = RFECV(estimator=RandomForestClassifier(n_estimators=10), step=1, cv=StratifiedKFold(5),\n",
    "               scoring='precision')\n",
    "        clf_labels = RandomForestClassifier(random_state=0, n_jobs=-1, n_estimators=10)\n",
    "        # clf_accuracy = BalancedBaggingClassifier(n_jobs=-1, random_state=0, n_estimators=100)\n",
    "        rfe_accuracy = RFECV(estimator=RandomForestClassifier(n_estimators=10), step=1, cv=StratifiedKFold(5), scoring='f1')\n",
    "        clf_accuracy = BalancedBaggingClassifier(random_state=0, n_jobs=-1, n_estimators=20)\n",
    "        # clf = BalancedRandomForestClassifier(random_state=0)  \n",
    "\n",
    "        # TODO don't eliminate all nans\n",
    "\n",
    "        #%%\n",
    "        # clf_labels.fit(train_labels[features], train_labels['correct'].astype(int))\n",
    "    #     print('Training label classifier...')\n",
    "        rfe_labels.fit(train_labels[train_labels['user_id'].isin(users_labels_train)][features].values, \n",
    "            train_labels[train_labels['user_id'].isin(users_labels_train)]['correct'].astype(int))\n",
    "\n",
    "        clf_labels.fit(train_labels[train_labels['user_id'].isin(users_labels_train)][features].values[:, rfe_labels.support_], \n",
    "            train_labels[train_labels['user_id'].isin(users_labels_train)]['correct'].astype(int))\n",
    "\n",
    "        train_labels = train_labels.join(pd.Series(\n",
    "            data=clf_labels.predict_proba(train_labels[train_labels['user_id'].isin(users_labels_test)][features].values[:, rfe_labels.support_])[:, 1], \n",
    "            index=train_labels[train_labels['user_id'].isin(users_labels_test)].index).rename('prob'), how='outer')\n",
    "\n",
    "        prob_hist_predictions = pd.DataFrame(train_labels[train_labels['user_id'].isin(users_labels_test)]\n",
    "            .groupby('user_id').apply(lambda x:\\\n",
    "            prob_hist(x['prob'].values)).rename('prob'))\n",
    "\n",
    "        prob_hist_predictions = prob_hist_predictions.join(user_quality_features)\n",
    "\n",
    "    #     print('Training accuracy classifier...')\n",
    "        rfe_accuracy.fit(np.concatenate((dearray(prob_hist_predictions['prob']), \n",
    "            prob_hist_predictions.drop(columns='prob').values), axis=1), \n",
    "            (y_train.loc[prob_hist_predictions.index] > 65).astype(int))\n",
    "\n",
    "        clf_accuracy.fit(np.concatenate((dearray(prob_hist_predictions['prob']), \n",
    "            prob_hist_predictions.drop(columns='prob').values), axis=1)[:, rfe_accuracy.support_], \n",
    "            (y_train.loc[prob_hist_predictions.index] > 65).astype(int))\n",
    "        #%%\n",
    "        # Probabililty correct\n",
    "        useful_test = test_labels[~pd.isna(test_labels[features]).any(axis=1)].copy()  # TODO don't eliminate all nans\n",
    "        # useful_test[features] = scaler.transform(useful_test[features])\n",
    "        # useful_test = useful_test.join(useful_test.apply(get_proximity_info, axis=1))\n",
    "        useful_test.loc[:, 'prob'] = clf_labels.predict_proba(useful_test[features].values[:, rfe_labels.support_])[:, 1]\n",
    "\n",
    "        # a = useful_test.groupby('user_id').apply(lambda x: prob_hist(x['prob']))\n",
    "        # break\n",
    "        #%%\n",
    "\n",
    "        # Now predict accuracy\n",
    "\n",
    "        def predict_accuracy(probs, features):\n",
    "            # fig = plt.figure()\n",
    "            # plt.xlim(0, 1)\n",
    "            # plt.hist(probs)\n",
    "\n",
    "            # selected_probs = probs[~np.isnan(probs)]\n",
    "            # return np.mean(selected_probs)\n",
    "            # return clf_accuracy.predict_proba([np.concatenate((prob_hist(probs), features))])[:, 1][0]\n",
    "            return clf_accuracy.predict([np.concatenate((prob_hist(probs), features))[rfe_accuracy.support_]])[0]\n",
    "\n",
    "    #     print('Making predictions...')\n",
    "        mean_probs = useful_test.groupby('user_id').apply(lambda x: predict_accuracy(x['prob'].values, user_quality_features.loc[x.name])).rename('predicted')\n",
    "\n",
    "        #%%\n",
    "        comparison = pd.DataFrame((mean_probs, y_test, pd.Series(np.full((len(y_test)), split_num), name='split_num', index=y_test.index))).T\n",
    "        comparison['prob_hist'] = useful_test.groupby('user_id').apply(lambda x: prob_hist(x['prob'].values))\n",
    "        comparison['probs'] = useful_test.groupby('user_id').apply(lambda x: x['prob'].values)\n",
    "        comparisons = comparisons.append(comparison)\n",
    "\n",
    "        #%%\n",
    "\n",
    "        split_num += 1\n",
    "\n",
    "        # sys.stderr.write(f'{split_num} / 5\\n')\n",
    "\n",
    "    comparisons['accuracy'] = (comparisons['accuracy'] > 65).astype(int)\n",
    "\n",
    "    mask = ~pd.isna(comparisons[['accuracy', 'predicted']]).any(axis=1)\n",
    "    # print(precision_score(comparisons['accuracy'][mask], comparisons['predicted'][mask]))\n",
    "    # print(recall_score(comparisons['accuracy'][mask], comparisons['predicted'][mask]))\n",
    "    # print(accuracy_score(comparisons['accuracy'][mask], comparisons['predicted'][mask]))\n",
    "    # print(confusion_matrix(comparisons['accuracy'][mask], comparisons['predicted'][mask]))\n",
    "\n",
    "    return precision_score(comparisons['accuracy'][mask], comparisons['predicted'][mask]), recall_score(comparisons['accuracy'][mask], comparisons['predicted'][mask])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_cell_magic('time', '', \"warnings.filterwarnings('ignore')\\ndef run_range(min_labels):\\n    try:\\n        print(min_labels)\\n        users_for_analysis = users.index[users['labels_validated'] > min_labels]\\n        users2 = users.loc[users_for_analysis]\\n        return get_p_r(users2, label_correctness)\\n    except Exception:\\n        return (-1, -1)\\n    \\nwith mp.Pool(4) as p:\\n    results = p.map(run_range, np.arange(1,75))\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.array(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_with_min = np.concatenate((np.arange(2, 76).reshape(-1, 1), results), axis=1)\n",
    "results_invalid = results_with_min[~(results == -1).any(axis=1)]\n",
    "results_df = pd.DataFrame(data=results_invalid, columns=('min', 'precision', 'recall')).set_index('min')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df.iloc[results_df.index < 45]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# plt.scatter(results_df.index, results_df['precision'])\n",
    "plt.scatter(results_df.index, results_df['recall'])\n",
    "# plt.scatter(results_df.index, 2/(1/results_df['precision'] + 1/results_df['recall']))\n",
    "plt.xlabel('min number of labels')\n",
    "plt.ylabel('recall')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[44]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_p_r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.vlines(np.arange(len(users['labels_validated'])), 0, np.sort(users['labels_validated'])[::-1], color='C0')\n",
    "plt.axis((0, None, 0, None))\n",
    "plt.xlabel('user number')\n",
    "plt.ylabel('number of validated labels')\n",
    "fig.savefig('a.svg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(users['labels_validated'])[::-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # DC Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Our attempts at classifying DC data. It didn't go very well (~55% p/r using the same features as Seattle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_features = pd.read_csv('all_users_dc_with_interaction.csv')\n",
    "dc_users = pd.read_csv('all_users_outputs_dc_with_interaction.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_features.set_index(\"Unnamed: 0\", inplace=True)\n",
    "dc_users.set_index(\"Unnamed: 0\", inplace=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "'R = ' + str(f'{pears[0]:.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in dc_features.columns:\n",
    "    if 'Tag' not in x:\n",
    "        plt.figure()\n",
    "        plt.title(x)\n",
    "        z = np.polyfit(dc_features[x],dc_users['f1'], 1)\n",
    "        w = np.poly1d(z)\n",
    "        pears = scipy.stats.pearsonr(dc_features[x],dc_users['f1'])\n",
    "        plt.scatter( dc_features[x],dc_users['f1'])\n",
    "        plt.plot(dc_features[x], w(dc_features[x]))\n",
    "        print('R = ' + str(f'{pears[0]:.2f}'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_users['f1'] = 2 / (1/dc_users['precision'] + 1/dc_users['recall'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_users['good'] = (dc_users['f1'] > 0.65)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(dc_users['f1'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons = pd.DataFrame()\n",
    "for train_index, test_index in KFold(n_splits=5, shuffle=True, random_state=0).split(dc_users.index):\n",
    "    X_train, X_test = dc_features.iloc[train_index], dc_features.iloc[test_index]\n",
    "    y_train, y_test = dc_users['good'].iloc[train_index], dc_users['good'].iloc[test_index]\n",
    "    \n",
    "#     rfe = RFECV(estimator=RandomForestClassifier(n_estimators=10), step=1, cv=StratifiedKFold(5),\n",
    "#            scoring='f1')\n",
    "    \n",
    "#     rfe.fit(X_train, y_train)\n",
    "#     clf = RandomForestClassifier(random_state=0, n_estimators=10)\n",
    "    clf = BalancedRandomForestClassifier(random_state=0, n_estimators=10)\n",
    "#     clf.fit(X_train.values[:, rfe.support_], y_train)\n",
    "    clf.fit(X_train.values, y_train)\n",
    "\n",
    "    comparisons = comparisons.append(pd.DataFrame(\n",
    "#         data=np.stack((clf.predict(X_test.values[:, rfe.support_]).reshape(-1), y_test.values.reshape(-1)), axis=1), \n",
    "        data=np.stack((clf.predict(X_test.values).reshape(-1), y_test.values.reshape(-1)), axis=1), \n",
    "        columns=('predicted', 'actual'))\n",
    "    )\n",
    "\n",
    "comparisons.reset_index(inplace=True, drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~pd.isna(comparisons[['actual', 'predicted']]).any(axis=1)\n",
    "print(precision_score(comparisons['actual'][mask], comparisons['predicted'][mask]))\n",
    "print(recall_score(comparisons['actual'][mask], comparisons['predicted'][mask]))\n",
    "print(accuracy_score(comparisons['actual'][mask], comparisons['predicted'][mask]))\n",
    "print(confusion_matrix(comparisons['actual'][mask], comparisons['predicted'][mask]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimal number of features : %s\" % str(rfe.support_))\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfe.grid_scores_) + 1), rfe.grid_scores_)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Analysis of number of panos vs. classifier precisison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_users_lists = pd.read_pickle('all_users_lists_new')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_users_lists['Average Pitch'] = all_users_lists['Average Pitch'].apply(list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_users_lists = all_users_lists.apply(lambda x: x.apply(np.array))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features_first_n_panos(all_users, n):\n",
    "    # generate features\n",
    "    return all_users.apply(lambda x: x.apply((lambda y: np.nanstd(y[:n])))).add_prefix('std_').join(\n",
    "    all_users.apply(lambda x: x.apply((lambda y: np.nanmean(y[:n])))).add_prefix('mean_')).join(\n",
    "    all_users.apply(lambda x: x.apply((lambda y: np.nanpercentile(y[:n], 50)))).add_prefix('median_')).join(\n",
    "    all_users.apply(lambda x: x.apply((lambda y: np.nanpercentile(y[:n], 25)))).add_prefix('25%ile_')).join(\n",
    "    all_users.apply(lambda x: x.apply((lambda y: np.nanpercentile(y[:n], 75)))).add_prefix('75%ile_'))\n",
    "#     return all_users.apply(lambda x: x.apply((lambda y: np.nanmean(y[:n])))).add_prefix('mean_')\n",
    "#     return all_users.apply(lambda x: x.apply((lambda y: np.nanpercentile(y[:n], 50)))).add_prefix('median_')\n",
    "    # drop all features with too few interactions\n",
    "#     return df[all_users['Zoom'].apply(len) >= n]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = generate_features_first_n_panos(all_users, 50).fillna(0)\n",
    "y = users['accuracy'].reindex(X.index)\n",
    "user_mask = ~pd.isna(y)\n",
    "X = X.loc[user_mask]\n",
    "y = (y.loc[user_mask] > 65).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isna(X).any().any()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_analysis(n):\n",
    "    X = generate_features_first_n_panos(all_users, n).fillna(0)\n",
    "    X = X[all_users['Zoom'].apply(len) >= n]\n",
    "    y = users['accuracy'].reindex(X.index)\n",
    "    user_mask = ~pd.isna(y)\n",
    "    X = X.loc[user_mask]\n",
    "    y = (y.loc[user_mask] > 65).astype(int)\n",
    "\n",
    "    comparisons = pd.DataFrame()\n",
    "    for train_index, test_index in KFold(n_splits=5, shuffle=True, random_state=0).split(X.index):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "#         rfe = RFECV(estimator=RandomForestClassifier(n_estimators=20, random_state=0), step=1, cv=StratifiedKFold(5),\n",
    "#                scoring='f1')\n",
    "        \n",
    "        rfe = RFECV(estimator=DecisionTreeClassifier(), step=1, cv=StratifiedKFold(5),\n",
    "               scoring='f1')\n",
    "\n",
    "        rfe.fit(X_train, y_train)\n",
    "    #     clf = RandomForestClassifier(random_state=0, n_estimators=10)\n",
    "#         clf = DecisionTreeClassifier(random_state=0)\n",
    "        clf = SVC(kernel='linear')\n",
    "        clf.fit(X_train.values[:, rfe.support_], y_train)\n",
    "\n",
    "        comparisons = comparisons.append(pd.DataFrame(\n",
    "            data=np.stack((clf.predict(X_test.values[:, rfe.support_]).reshape(-1), y_test.values.reshape(-1)), axis=1), \n",
    "#             data=np.stack((rfe.predict(X_test.values).reshape(-1), y_test.values.reshape(-1)), axis=1), \n",
    "            columns=('predicted', 'actual'))\n",
    "        )\n",
    "\n",
    "    comparisons.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "    return (sklearn.metrics.precision_score(comparisons['actual'], comparisons['predicted']), \n",
    "            sklearn.metrics.recall_score(comparisons['actual'], comparisons['predicted']),\n",
    "            sklearn.metrics.confusion_matrix(comparisons['actual'], comparisons['predicted']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_range = np.arange(5, 500, 20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp.Pool(4) as p:\n",
    "    analysis_results = p.map(run_analysis, analysis_range)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_array = np.array(analysis_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(analysis_range, analysis_array[:, 0], label='precision')\n",
    "plt.scatter(analysis_range, analysis_array[:, 1], label='recall')\n",
    "plt.xlabel('number of panos')\n",
    "plt.legend()\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
